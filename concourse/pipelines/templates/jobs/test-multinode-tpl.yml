{# Template for a PXF multi-node test job definition.
 #
 # Expects a dictionary with the name 'x' to be set as defined in macros.j2 file and these additional fields:
 #
 # x.use_fdw    - whether to use FDW extension for testing (otherwise the external table extension will be used)
 # x.use_impers - whether to use impersonation for the test job
 # x.target     - the target external system type
 # x.distro     - the distro to use for a singlecluster image
 # x.passed     - an array of upstream job names that the artifacts need to pass before this job (optional)
 # x.with_upgrade - 'true' is the job should include an upgrade task from PXF5 to PXF6
 #}

{% set extension = 'fdw' if x.use_fdw == 'true' else 'ext' %}
{% if x.use_impers == 'true' %}
    {% set feature = 'ipa' %}
    {% set ccp_default_conf = 'true' %}
    {% set test_groups = 'security,proxySecurity,proxySecurityIpa,multiClusterSecurity' %}
{% else %}
    {% set feature = 'no-impers' %}
    {% set ccp_default_conf = 'false' %}
    {% set test_groups = 'security,multiClusterSecurity' %}
{% endif %}

{# if x.passed was not provided, make it a build job on the same platform #}
{% set passed = x.passed if x.passed else '[build-pxf-gp' ~ x.gp_ver ~ '-' ~ x.build_platform ~ ']' %}

{% do x.update({'job_name': 'test-pxf-gp' ~ x.gp_ver ~ '-' ~ extension ~ '-' ~ x.target ~ '-secure-multi-' ~ feature ~ '-' ~ x.test_platform}) %}
- name: [[x.job_name]]
  max_in_flight: 2
  plan:
  - in_parallel:
    - get: pxf_src
      resource: pxf-src
      passed: [[passed]]
      trigger: true
{% if x.with_upgrade == 'true' %}
    - get: pxf_artifact
      resource: pxf5-gp6-el7-released   # for upgrade test
{% endif %}
    - get: pxf_tarball
      resource: [[x.pxf_tarball_resource_name]]
      passed: [[passed]]
    - get: gpdb_package
      resource: [[x.gpdb_package_resource_name]]
      passed: [[passed]]
    - get: [[x.test_image_resource_name]]
      {# if the image was also used to build the artifact, add a 'passed' condition #}
{% if [[x.test_platform]] == [[x.build_platform]] %}
      passed: [[passed]]
{% endif %}
    - get: ccp_src
      resource: ccp-src
    - get: ccp-7-image
    - get: pxf-automation-dependencies
    - get: singlecluster
      resource: singlecluster-[[x.distro]]
  - in_parallel:
    - do:
      - put: terraform-gpdb
        params:
          action: create
          delete_on_failure: true
          generate_random_name: true
          terraform_source: ccp_src/google/
          vars:
            PLATFORM: [[x.test_platform]]-gpdb[[x.gp_ver]]
            WITH_MIRRORS: [[ccp_default_conf]]
            number_of_nodes: ((number_of_gpdb_nodes))
            extra_nodes: 1
            segments_per_host: 4
            instance_type: n1-standard-4
            ccp_reap_minutes: 180
            standby_master: [[ccp_default_conf]]
      - task: generate-greenplum-cluster
        input_mapping:
          gpdb_rpm: gpdb_package
          terraform: terraform-gpdb
        file: ccp_src/ci/tasks/gen_cluster.yml
        image: ccp-7-image
        params:
          AWS_ACCESS_KEY_ID: ((tf-machine-access-key-id))
          AWS_SECRET_ACCESS_KEY: ((tf-machine-secret-access-key))
          AWS_DEFAULT_REGION: ((ud/common/aws-region))
          BUCKET_PATH: ((tf-bucket-path))
          BUCKET_NAME: ((ud/pxf/common/tf-bucket-name))
          PLATFORM: [[x.test_platform]]-gpdb[[x.gp_ver]]
          CLOUD_PROVIDER: google
          GPDB_RPM: true
      - in_parallel:
        - task: initialize-greenplum
          file: ccp_src/ci/tasks/gpinitsystem.yml
          params:
            PLATFORM: [[x.test_platform]]-gpdb[[x.gp_ver]]
{% if ccp_default_conf == 'false' %}
            POSTGRES_CONF_ADDONS:
            - wal_level=minimal
            - max_wal_senders=0
            - gp_dispatch_keepalives_idle=30
            - gp_dispatch_keepalives_interval=10
            - gp_dispatch_keepalives_count=4
{% endif %}
        - task: install-hadoop
          file: pxf_src/concourse/tasks/install_hadoop.yml
          image: [[x.test_image_resource_name]]
          params:
            ACCESS_KEY_ID: ((tf-machine-access-key-id))
            SECRET_ACCESS_KEY: ((tf-machine-secret-access-key))
            IMPERSONATION: ((enable-impersonation-multinode))
    - task: generate-hadoop-cluster-1
      file: pxf_src/concourse/tasks/install_dataproc.yml
      output_mapping:
        dataproc_env_files: dataproc_1_env_files
      params:
        GOOGLE_CREDENTIALS: ((ud/pxf/secrets/ccp-ci-service-account-key))
        GOOGLE_PROJECT_ID: ((ud/pxf/common/google-project-id))
        GOOGLE_ZONE: ((ud/pxf/common/google-zone))
        IMAGE_VERSION: ((dataproc-image-version))
        KERBEROS: ((kerberos-enabled))
        ccp_reap_minutes: 180
    - task: generate-hadoop-cluster-2
      file: pxf_src/concourse/tasks/install_dataproc.yml
      output_mapping:
        dataproc_env_files: dataproc_2_env_files
      params:
        GOOGLE_CREDENTIALS: ((ud/pxf/secrets/kerberos-ccp-ci-service-account-key))
        GOOGLE_PROJECT_ID: ((ud/pxf/common/kerberos-google-project-id))
        GOOGLE_ZONE: ((ud/pxf/common/kerberos-google-zone))
        HADOOP_USER: gpuser
        IMAGE_VERSION: ((dataproc-image-version))
        INITIALIZATION_SCRIPT: gs://data-gpdb-ud-kerberos-scripts/scripts/initialization-for-kerberos.sh
        INSTANCE_TAGS: bosh-network,data-gpdb-ud-access
        KERBEROS: ((kerberos-enabled))
        KEY: dataproc-kerberos-key
        KEYRING: dataproc-kerberos
        ccp_reap_minutes: 180
        NO_ADDRESS: false
        PROXY_USER: gpuser
        SECRETS_BUCKET: ((ud/pxf/secrets/kerberos-pxf-secrets-bucket-name))
{% if x.use_impers == 'true' %}
    - do:   # Generate IPA Hadoop cluster
      - put: terraform-ipa-hadoop
        params:
          action: create
          generate_random_name: true
          terraform_source: pxf_src/concourse/terraform/ipa-multinode-hadoop
          vars:
            gcp_project: ((ud/pxf/common/ipa-google-project-id))
      - task: generate-multinode-hadoop-cluster
        file: pxf_src/concourse/tasks/install_multinode_hadoop.yml
        image: [[x.test_image_resource_name]]
        input_mapping:
          terraform_ipa_hadoop: terraform-ipa-hadoop
        params:
          ANSIBLE_VAR_gcp_storage_bucket: ((ud/pxf/common/build-resources-bucket-name))
          ANSIBLE_VAR_ipa_password: ((ud/pxf/secrets/ipa-password))
          ANSIBLE_VAR_ssl_store_password: ((ud/pxf/secrets/ssl-store-password))
{% endif %}{# end of conditional IPA cluster generation #}
{% if x.with_upgrade == 'true' %}
  - task: setup-pxf-5-latest
    input_mapping:
      terraform: terraform-gpdb
      dataproc_env_files: dataproc_1_env_files
      pxf_tarball: pxf_tarball_ignore   # do not install from the artifact
    file: pxf_src/concourse/tasks/install_pxf_on_ccp.yml
    image: ccp-7-image
    params:
      GOOGLE_PROJECT_ID: ((ud/pxf/common/google-project-id))
      GP_VER: [[x.gp_ver]]
      IMPERSONATION: true
      INSTALL_GPHDFS: false
      KERBEROS: ((kerberos-enabled))
      PXF_JVM_OPTS: ((pxf-jvm-opts))
      PXF_VERSION: 5
  - task: upgrade-to-pxf-6
    input_mapping:
      terraform: terraform-gpdb
    file: pxf_src/concourse/tasks/upgrade_pxf_on_ccp.yml
    image: ccp-7-image
    params:
      GP_VER: [[x.gp_ver]]
      PXF_BASE_DIR: /home/gpadmin/pxf-boot
{% else %}
  - task: setup-pxf
    input_mapping:
      terraform: terraform-gpdb
      dataproc_env_files: dataproc_1_env_files
    file: pxf_src/concourse/tasks/install_pxf_on_ccp.yml
    image: ccp-7-image
    params:
      IMPERSONATION: [[x.use_impers]]
      INSTALL_GPHDFS: false
      GP_VER: [[x.gp_ver]]
      KERBEROS: ((kerberos-enabled))
      PXF_JVM_OPTS: ((pxf-jvm-opts))
      PXF_BASE_DIR: /home/gpadmin/pxf
{% if x.use_impers == 'true' %}
      PXF_BASE_DIR: /home/gpadmin/pxf-boot
{% endif %}
{% endif %}{# end of upgrade if/else block #}
  - task: [[x.job_name]]
    input_mapping:
      dataproc_env_files: dataproc_1_env_files
    image: [[x.test_image_resource_name]]
    file: pxf_src/concourse/tasks/test_pxf_on_ccp.yml
    attempts: 2
    params:
      GOOGLE_PROJECT_ID: ((ud/pxf/common/google-project-id))
      ACCESS_KEY_ID: ((tf-machine-access-key-id))
      SECRET_ACCESS_KEY: ((tf-machine-secret-access-key))
      HIVE_VERSION: 2
      IMPERSONATION: [[x.use_impers]]
      KERBEROS: ((kerberos-enabled))
      GP_VER: [[x.gp_ver]]
      GROUP: [[test_groups]]
      PXF_JVM_OPTS: ((pxf-jvm-opts))
{% if x.use_impers == 'true' %}
      PXF_BASE_DIR: /home/gpadmin/pxf-boot
{% endif %}
{% if x.test_platform == 'centos7' %}
  - task: test-pxf-cli
    image: [[x.test_image_resource_name]]
    file: pxf_src/concourse/tasks/test_pxf_cli.yml
{% endif %}
  on_success:
    in_parallel:
      steps:
      - *destroy_dataproc_1
      - *destroy_dataproc_2
      - *destroy_gpdb_cluster
{% if x.use_impers == 'true' %}
      - *destroy_ipa-hadoop_cluster
{% endif %}
{% if gchat_notification %}
  <<: *gchat_alert
{% endif %}
